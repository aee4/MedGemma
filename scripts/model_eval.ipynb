{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13842792,
          "sourceType": "datasetVersion",
          "datasetId": 8816807
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "model_eval",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aee4/MedGemma/blob/main/scripts/model_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "i3-PccFVoBzq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "emmanueleyramagbetor_dataset_eczema_path = kagglehub.dataset_download('emmanueleyramagbetor/dataset-eczema')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Huw5tsUMoBzs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install datasets scikit-learn peft transformers\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T00:41:28.10068Z",
          "iopub.execute_input": "2025-11-25T00:41:28.101309Z",
          "iopub.status.idle": "2025-11-25T00:43:26.391377Z",
          "shell.execute_reply.started": "2025-11-25T00:41:28.101284Z",
          "shell.execute_reply": "2025-11-25T00:43:26.390676Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Ydg2cJ8SoBzs",
        "outputId": "b41de783-261a-4720-8053-ef4a6abcca05"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-3hn9us80/unsloth_de77b9d3ae1c4122bd3bf635ef5ae1b5\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-3hn9us80/unsloth_de77b9d3ae1c4122bd3bf635ef5ae1b5\n  Resolved https://github.com/unslothai/unsloth.git to commit 872e191494104d177d854a2e2b430a03a193d4e2\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting unsloth_zoo>=2025.11.4 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading unsloth_zoo-2025.11.4-py3-none-any.whl.metadata (32 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\nCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.53.3)\nCollecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (7.1.3)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.33.0)\nRequirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.36.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\nCollecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.0)\nCollecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\nCollecting multiprocess<0.70.17 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.3)\nCollecting torchao>=0.13.0 (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.0)\nCollecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\nCollecting msgspec (from unsloth_zoo>=2025.11.4->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading msgspec-0.20.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.2.0)\nCollecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\nCollecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading transformers-4.57.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.22.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.3)\nDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.11.4-py3-none-any.whl (283 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.5/283.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.57.2-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.20.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (219 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for unsloth: filename=unsloth-2025.11.3-py3-none-any.whl size=359940 sha256=b1b5d0d1e1e59f6f6f49a985722532a37398dda1bbecec8d354392b247b1eb10\n  Stored in directory: /tmp/pip-ephem-wheel-cache-fur1bd27/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\nSuccessfully built unsloth\nInstalling collected packages: torchao, unsloth, shtab, pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, tokenizers, nvidia-cusolver-cu12, cut_cross_entropy, transformers, datasets, trl, unsloth_zoo, bitsandbytes\n  Attempting uninstall: torchao\n    Found existing installation: torchao 0.10.0\n    Uninstalling torchao-0.10.0:\n      Successfully uninstalled torchao-0.10.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.18\n    Uninstalling multiprocess-0.70.18:\n      Successfully uninstalled multiprocess-0.70.18\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.10.0\n    Uninstalling fsspec-2025.10.0:\n      Successfully uninstalled fsspec-2025.10.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.1\n    Uninstalling datasets-4.4.1:\n      Successfully uninstalled datasets-4.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.9.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 fsspec-2025.9.0 msgspec-0.20.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-22.0.0 shtab-1.8.0 tokenizers-0.22.1 torchao-0.14.1 transformers-4.57.2 trl-0.24.0 tyro-0.9.35 unsloth-2025.11.3 unsloth_zoo-2025.11.4\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.1.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_files = {\n",
        "    \"test\": \"/kaggle/input/dataset-eczema/test.csv\",\n",
        "}\n",
        "\n",
        "dataset = load_dataset(\"csv\", data_files=data_files)\n",
        "test_ds = dataset[\"test\"]\n",
        "test_ds\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T00:43:53.859671Z",
          "iopub.execute_input": "2025-11-25T00:43:53.860257Z",
          "iopub.status.idle": "2025-11-25T00:43:58.182378Z",
          "shell.execute_reply.started": "2025-11-25T00:43:53.860226Z",
          "shell.execute_reply": "2025-11-25T00:43:58.181615Z"
        },
        "id": "-ftoFbz2oBzt",
        "outputId": "6989bef7-4872-429c-9906-db1491bfb322",
        "colab": {
          "referenced_widgets": [
            "8f48c59c1a96416bab3a802406788968"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f48c59c1a96416bab3a802406788968"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 2000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_for_eval(example):\n",
        "    prompt = f\"Description: {example['text']}\\n\\nAnswer the correct class (0 or 1).\"\n",
        "    example[\"prompt\"] = prompt\n",
        "    return example\n",
        "\n",
        "test_ds = test_ds.map(format_for_eval)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T00:44:17.874431Z",
          "iopub.execute_input": "2025-11-25T00:44:17.875369Z",
          "iopub.status.idle": "2025-11-25T00:44:17.963828Z",
          "shell.execute_reply.started": "2025-11-25T00:44:17.875328Z",
          "shell.execute_reply": "2025-11-25T00:44:17.963004Z"
        },
        "id": "aqZBNEgLoBzt",
        "outputId": "20a0cead-11a5-475c-914e-5e1b2115ef75",
        "colab": {
          "referenced_widgets": [
            "3517c6a151124023b6f169ad01bd29a9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3517c6a151124023b6f169ad01bd29a9"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "base_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "base_model, base_processor = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_id,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "base_processor.tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T00:44:32.270889Z",
          "iopub.execute_input": "2025-11-25T00:44:32.271588Z",
          "iopub.status.idle": "2025-11-25T00:45:34.736368Z",
          "shell.execute_reply.started": "2025-11-25T00:44:32.271562Z",
          "shell.execute_reply": "2025-11-25T00:45:34.735667Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "ZTxkjffaoBzt",
        "outputId": "78e878f2-88c4-4a48-c20c-683f9beab1de",
        "colab": {
          "referenced_widgets": [
            "70f7a85154c94eb88d15e04c72e01747",
            "fecf77ed4d204548b09cd694e6e3ef13",
            "b34cd924ace0468fb43db464fbb4cd39",
            "cf9c0de3108f4f32bbf67f3777cf1f24",
            "5af1a8b41ae7450c83d9a068a9d1d7f6",
            "456859ce61264d33a4bbfd9dd853049e",
            "abd45d6e4ba64d9d841ef49768e39c06",
            "405b19d651054d9481a1cf42eb04640f",
            "5b998c1723f74086a1eb3f4f1e399337",
            "375db41031e244f1a1eaed949300bd6d"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2025-11-25 00:44:37.504121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764031477.691277      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764031477.743108      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.11.3: Fast Gemma3 patching. Transformers: 4.57.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\nUnsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70f7a85154c94eb88d15e04c72e01747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fecf77ed4d204548b09cd694e6e3ef13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b34cd924ace0468fb43db464fbb4cd39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "chat_template.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf9c0de3108f4f32bbf67f3777cf1f24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5af1a8b41ae7450c83d9a068a9d1d7f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "456859ce61264d33a4bbfd9dd853049e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abd45d6e4ba64d9d841ef49768e39c06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "405b19d651054d9481a1cf42eb04640f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b998c1723f74086a1eb3f4f1e399337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "375db41031e244f1a1eaed949300bd6d"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def predict(model, processor, text):\n",
        "    inputs = processor(text=[text], return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(**inputs, max_new_tokens=5)\n",
        "    out = processor.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract final digit (very reliable for binary)\n",
        "    if \"1\" in out[-3:]:\n",
        "        return 1\n",
        "    if \"0\" in out[-3:]:\n",
        "        return 0\n",
        "    return 0\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T00:45:54.7039Z",
          "iopub.execute_input": "2025-11-25T00:45:54.704183Z",
          "iopub.status.idle": "2025-11-25T00:45:54.709096Z",
          "shell.execute_reply.started": "2025-11-25T00:45:54.70416Z",
          "shell.execute_reply": "2025-11-25T00:45:54.708391Z"
        },
        "id": "JO0xL1KSoBzt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def evaluate_model(model, processor, ds):\n",
        "    preds, labels = [], []\n",
        "\n",
        "    for ex in ds:\n",
        "        pred = predict(model, processor, ex[\"prompt\"])\n",
        "        preds.append(pred)\n",
        "        labels.append(int(ex[\"label\"]))\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"binary\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T00:46:10.530016Z",
          "iopub.execute_input": "2025-11-25T00:46:10.530769Z",
          "iopub.status.idle": "2025-11-25T00:46:10.535781Z",
          "shell.execute_reply.started": "2025-11-25T00:46:10.530742Z",
          "shell.execute_reply": "2025-11-25T00:46:10.535027Z"
        },
        "id": "9I7M7xh1oBzt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating BASE medgemma-4b-it...\")\n",
        "base_scores = evaluate_model(base_model, base_processor, test_ds)\n",
        "base_scores\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T00:46:21.476975Z",
          "iopub.execute_input": "2025-11-25T00:46:21.477484Z",
          "iopub.status.idle": "2025-11-25T01:10:41.303888Z",
          "shell.execute_reply.started": "2025-11-25T00:46:21.477463Z",
          "shell.execute_reply": "2025-11-25T01:10:41.303163Z"
        },
        "id": "MAYWs2YzoBzu",
        "outputId": "fe60ba37-80c0-44d7-d6c8-d9e0468d8bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Evaluating BASE medgemma-4b-it...\n",
          "output_type": "stream"
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'accuracy': 0.655,\n 'precision': 0.9112903225806451,\n 'recall': 0.4708333333333333,\n 'f1': 0.6208791208791208}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T01:11:46.07008Z",
          "iopub.execute_input": "2025-11-25T01:11:46.070382Z",
          "iopub.status.idle": "2025-11-25T01:11:46.085856Z",
          "shell.execute_reply.started": "2025-11-25T01:11:46.070358Z",
          "shell.execute_reply": "2025-11-25T01:11:46.084986Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "3_xAqHo4oBzu",
        "outputId": "95a1e327-433c-49c5-9437-dee0646c8867",
        "colab": {
          "referenced_widgets": [
            "f917ecb830a44463ac44c85e7f500ef5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f917ecb830a44463ac44c85e7f500ef5"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "lora_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"aee4/medgemma-eczema\",\n",
        ")\n",
        "lora_model.eval()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T01:12:03.495712Z",
          "iopub.execute_input": "2025-11-25T01:12:03.495991Z",
          "iopub.status.idle": "2025-11-25T01:12:06.496372Z",
          "shell.execute_reply.started": "2025-11-25T01:12:03.49597Z",
          "shell.execute_reply": "2025-11-25T01:12:06.495747Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "JX56W_A6oBzu",
        "outputId": "f12f9c0b-ddc3-41be-bef2-b2f774639497",
        "colab": {
          "referenced_widgets": [
            "539bb3ee111343ffa3a80a23ac749ec3",
            "78671c3db9f844eb811edfc0b4047476"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "539bb3ee111343ffa3a80a23ac749ec3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/154M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78671c3db9f844eb811edfc0b4047476"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma3ForConditionalGeneration(\n      (model): Gemma3Model(\n        (vision_tower): SiglipVisionModel(\n          (vision_model): SiglipVisionTransformer(\n            (embeddings): SiglipVisionEmbeddings(\n              (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n              (position_embedding): Embedding(4096, 1152)\n            )\n            (encoder): SiglipEncoder(\n              (layers): ModuleList(\n                (0-15): 16 x SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear(\n                      (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n                (16): SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n                (17-18): 2 x SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n                (19-20): 2 x SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n                (21-22): 2 x SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n                (23): SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n                (24-25): 2 x SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear(\n                      (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n                (26): SiglipEncoderLayer(\n                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (self_attn): SiglipAttention(\n                    (k_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (q_proj): lora.Linear4bit(\n                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (out_proj): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                  (mlp): SiglipMLP(\n                    (activation_fn): GELUTanh()\n                    (fc1): lora.Linear(\n                      (base_layer): Linear(in_features=1152, out_features=4304, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=1152, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=4304, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (fc2): lora.Linear(\n                      (base_layer): Linear(in_features=4304, out_features=1152, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=4304, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=1152, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                  )\n                )\n              )\n            )\n            (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n          )\n        )\n        (multi_modal_projector): Gemma3MultiModalProjector(\n          (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n          (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n        )\n        (language_model): Gemma3TextModel(\n          (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n          (layers): ModuleList(\n            (0): Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear(\n                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n            (1): Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n            (2): Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n            (3): Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n            (4-5): 2 x Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear(\n                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n            (6): Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n            (7-9): 3 x Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n            (10-33): 24 x Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): GELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n          )\n          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (rotary_emb): Gemma3RotaryEmbedding()\n          (rotary_emb_local): Gemma3RotaryEmbedding()\n        )\n      )\n      (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating BASE + ADAPTER (finetuned)...\")\n",
        "lora_scores = evaluate_model(lora_model, base_processor, test_ds)\n",
        "lora_scores\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-25T01:12:24.291503Z",
          "iopub.execute_input": "2025-11-25T01:12:24.292178Z",
          "iopub.status.idle": "2025-11-25T01:24:34.015274Z",
          "shell.execute_reply.started": "2025-11-25T01:12:24.292142Z",
          "shell.execute_reply": "2025-11-25T01:24:34.014667Z"
        },
        "id": "yudMNCp6oBzu",
        "outputId": "61d5eb7d-a6da-43c3-e48c-de6f0ece1834"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Evaluating BASE + ADAPTER (finetuned)...\n",
          "output_type": "stream"
        },
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'accuracy': 0.683,\n 'precision': 0.6543075245365322,\n 'recall': 1.0,\n 'f1': 0.7910349373764008}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    }
  ]
}